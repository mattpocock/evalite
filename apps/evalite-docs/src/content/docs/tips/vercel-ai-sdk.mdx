---
title: Vercel AI SDK
---

import { Aside } from "@astrojs/starlight/components";

Use Evalite with Vercel's [AI SDK](https://sdk.vercel.ai/docs/introduction) to trace LLM calls and test AI-powered features.

## Automatic Tracing with `traceAISDKModel`

Wrap your AI SDK model in `traceAISDKModel` to automatically track all LLM calls:

```ts
// my-eval.eval.ts

import { openai } from "@ai-sdk/openai";
import { streamText } from "ai";
import { evalite } from "evalite";
import { faithfulness } from "evalite/scorers";
import { traceAISDKModel } from "evalite/ai-sdk";

evalite("Test Capitals", {
  data: async () => [
    {
      input: `What's the capital of France?`,
      expected: {
        groundTruth: [`France's capital city is Paris.`],
      },
    },
    {
      input: `What's the capital of Germany?`,
      expected: {
        groundTruth: [`Germany's capital city is Berlin.`],
      },
    },
  ],
  task: async (input) => {
    const result = streamText({
      model: traceAISDKModel(openai("gpt-4o-mini")),
      system: `
        Answer the question concisely. Answer in as few words as possible.
        Remove full stops from the end of the output.
        If the country has no capital, return '<country> has no capital'.
        If the country does not exist, return 'Unknown'.
      `,
      prompt: input,
    });

    return await result.text;
  },
  scorers: [
    {
      scorer: ({ input, output, expected }) =>
        faithfulness({
          question: input,
          answer: output,
          groundTruth: expected.groundTruth,
          model: openai("gpt-4o-mini"),
        }),
    },
  ],
});
```

<Aside>

`traceAISDKModel` is a no-op in production, so you can leave it in your code without worrying about performance.

</Aside>
