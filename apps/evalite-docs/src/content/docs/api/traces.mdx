---
title: Traces
---

Track nested LLM calls and intermediate steps within your evaluations.

## Overview

Traces allow you to record individual LLM calls or processing steps that occur during task execution. They appear in the Evalite UI alongside your main input/output, helping you debug and understand the full execution flow.

## Functions

### `reportTrace()`

Manually report a trace for custom LLM calls or processing steps.

**Signature:**

```typescript
reportTrace(trace: {
  input: unknown;
  output: unknown;
  usage?: {
    inputTokens: number;
    outputTokens: number;
    totalTokens: number;
  };
  start?: number;
  end?: number;
}): void
```

**Parameters:**

- `input` - The input to the operation (e.g., prompt, messages)
- `output` - The output from the operation (e.g., LLM response)
- `usage` (optional) - Token usage statistics
- `start` (optional) - Start timestamp (milliseconds). Defaults to current time.
- `end` (optional) - End timestamp (milliseconds). Defaults to current time.

**Example:**

```typescript
import { evalite } from "evalite";
import { reportTrace } from "evalite/traces";

evalite("Multi-Step Analysis", {
  data: [{ input: "Analyze this text" }],
  task: async (input) => {
    // First LLM call
    reportTrace({
      input: { prompt: "Summarize: " + input },
      output: { text: "Summary of the text" },
      usage: {
        inputTokens: 50,
        outputTokens: 20,
        totalTokens: 70,
      },
    });

    // Second LLM call
    reportTrace({
      input: { prompt: "Translate to Spanish: Summary of the text" },
      output: { text: "Resumen del texto" },
      usage: {
        inputTokens: 30,
        outputTokens: 15,
        totalTokens: 45,
      },
    });

    return "Final result";
  },
  scorers: [],
});
```

**Usage with timestamps:**

```typescript
const start = performance.now();
const result = await callLLM(input);
const end = performance.now();

reportTrace({
  input,
  output: result,
  start,
  end,
});
```

### `traceAISDKModel()`

Automatically trace all calls made with a Vercel AI SDK model.

**Signature:**

```typescript
traceAISDKModel(model: LanguageModelV2): LanguageModelV2
```

**Parameters:**

- `model` - A Vercel AI SDK language model (from `@ai-sdk/openai`, etc.)

**Returns:** A wrapped model that automatically reports traces.

**Example:**

```typescript
import { evalite } from "evalite";
import { traceAISDKModel } from "evalite/ai-sdk";
import { openai } from "@ai-sdk/openai";
import { generateText } from "ai";

// Wrap your model
const tracedModel = traceAISDKModel(openai("gpt-4"));

evalite("AI SDK Eval", {
  data: [{ input: "Hello" }],
  task: async (input) => {
    // All calls with this model are automatically traced
    const result = await generateText({
      model: tracedModel,
      prompt: input,
    });

    return result.text;
  },
  scorers: [],
});
```

**With streaming:**

```typescript
import { streamText } from "ai";

const tracedModel = traceAISDKModel(openai("gpt-4"));

evalite("Streaming Eval", {
  data: [{ input: "Hello" }],
  task: async (input) => {
    const result = await streamText({
      model: tracedModel,
      prompt: input,
    });

    // Process the stream before returning
    const text = await result.text;
    return text;
  },
  scorers: [],
});
```

## Enabling Traces

Traces are only recorded when the `EVALITE_REPORT_TRACES` environment variable is set:

```bash
EVALITE_REPORT_TRACES=true evalite watch
```

Or in your `.env` file:

```
EVALITE_REPORT_TRACES=true
```

This prevents unnecessary overhead when traces aren't needed.

## What Gets Traced

### With `reportTrace()`

You control exactly what gets traced:

```typescript
reportTrace({
  input: "Whatever you want to log",
  output: { any: "data structure" },
});
```

### With `traceAISDKModel()`

Automatically traces:

- Full prompt/messages
- Model responses (text and tool calls)
- Token usage
- Timing information

## Viewing Traces in the UI

Traces appear in the Evalite UI under each test case:

1. Navigate to an eval result
2. Click on a specific test case
3. View the "Traces" section to see all nested calls
4. Inspect input, output, and timing for each trace

## Complete Example

```typescript
import { evalite } from "evalite";
import { reportTrace, traceAISDKModel } from "evalite/traces";
import { openai } from "@ai-sdk/openai";
import { generateText } from "ai";

const tracedModel = traceAISDKModel(openai("gpt-4"));

evalite("Research Agent", {
  data: [
    {
      input: "What is the capital of France?",
      expected: "Paris",
    },
  ],
  task: async (input) => {
    // Step 1: Extract intent (manually traced)
    const intent = await extractIntent(input);
    reportTrace({
      input: { query: input },
      output: { intent },
    });

    // Step 2: Generate response (automatically traced via AI SDK)
    const result = await generateText({
      model: tracedModel,
      prompt: `Answer this question: ${input}`,
    });

    // Step 3: Format result (manually traced)
    const formatted = formatResponse(result.text);
    reportTrace({
      input: { raw: result.text },
      output: { formatted },
    });

    return formatted;
  },
  scorers: [
    {
      name: "Exact Match",
      scorer: ({ output, expected }) => {
        return output === expected ? 1 : 0;
      },
    },
  ],
});
```

## Best Practices

1. **Use `traceAISDKModel()` for AI SDK calls** - Automatic tracing with rich context
2. **Use `reportTrace()` for custom logic** - Track non-LLM steps (parsing, validation, etc.)
3. **Include usage data when available** - Helps track costs and performance
4. **Keep trace data relevant** - Don't trace every small operation, focus on meaningful steps
5. **Enable only when needed** - Use `EVALITE_REPORT_TRACES=true` during development/debugging

## Troubleshooting

### Traces not appearing

Make sure `EVALITE_REPORT_TRACES=true` is set:

```bash
EVALITE_REPORT_TRACES=true evalite watch
```

### Error: "reportTrace must be called inside an evalite eval"

`reportTrace()` can only be called within the `task` function of an eval:

```typescript
// ✅ Correct
evalite("My Eval", {
  data: [{ input: "test" }],
  task: async (input) => {
    reportTrace({ input, output: "result" }); // Works
    return "result";
  },
});

// ❌ Wrong
reportTrace({ input: "test", output: "result" }); // Outside eval
```

## See Also

- [Adding Traces Guide](/tips/adding-traces) - Overview and examples
- [Vercel AI SDK Integration](/tips/vercel-ai-sdk) - Using AI SDK with Evalite
- [evalite()](/api/evalite) - Main evaluation function
